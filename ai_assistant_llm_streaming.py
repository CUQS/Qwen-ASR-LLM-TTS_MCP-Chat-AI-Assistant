import sys
import os
import re
import threading
import queue
import keyboard
import ollama
import subprocess
import asyncio  # MCP æ˜¯å¼‚æ­¥çš„
import tempfile
import numpy as np
import sounddevice as sd
import soundfile as sf
import torch
from PyQt6.QtWidgets import (QApplication, QWidget, QVBoxLayout, QTextEdit, 
                             QLineEdit, QPushButton, QHBoxLayout, QLabel, QCheckBox)
from PyQt6.QtCore import QObject, pyqtSignal, Qt
from PyQt6.QtGui import QTextDocument
import html

# --- ASR / TTS ---
from qwen_asr import Qwen3ASRModel
from qwen_tts import Qwen3TTSModel
from kokoro import KModel, KPipeline

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# --- Web Chat é›†æˆ ---
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'webpage_chat'))
from server import set_assistant, start_server as start_web_server, broadcast_message as web_broadcast

# --- é…ç½®åŒº ---
REMOTE_OLLAMA_HOST = "http://192.168.40.12:11434" 
MODEL_NAME = "dengcao/Qwen3-30B-A3B-Instruct-2507"

# TTS å¼•æ“é€‰æ‹©: "qwen" æˆ– "kokoro"
TTS_ENGINE = "kokoro"  # è®¾ä¸º "kokoro" å¯ä½¿ç”¨ Kokoro TTS

# ASR / TTS æ¨¡å‹é…ç½®
ASR_MODEL_ID = "Qwen/Qwen3-ASR-0.6B"
TTS_MODEL_ID = "Qwen/Qwen3-TTS-12Hz-0.6B-CustomVoice"
TTS_SPEAKER = "Serena"
TTS_LANGUAGE = "Chinese"
TTS_TOKEN_MAX_NUM = 100  # TTS å•å¥æœ€å¤§å­—ç¬¦æ•°ï¼Œè¶…è¿‡åˆ™ç»§ç»­æ‹†åˆ†
RECORD_SAMPLE_RATE = 16000  # ASR è¦æ±‚ 16kHz

# Kokoro TTS é…ç½®
KOKORO_REPO_ID = 'hexgrad/Kokoro-82M-v1.1-zh'
KOKORO_SAMPLE_RATE = 24000
KOKORO_VOICE = 'zf_001'  # zf_001 å¥³å£°, zm_010 ç”·å£°

# --- å¥å­æ‹†åˆ†å·¥å…· ---
_PUNCT_PATTERN = re.compile(r'(?<=[ã€‚ï¼ï¼Ÿï¼›\n!\?;])')
_SUB_PUNCT_PATTERN = re.compile(r'(?<=[ï¼Œ,ã€ï¼š:\-â€”])')

def split_sentences_for_tts(text: str, max_len: int = TTS_TOKEN_MAX_NUM) -> list[str]:
    """æŒ‰æ ‡ç‚¹å°†æ–‡æœ¬æ‹†åˆ†ä¸ºé€‚åˆ TTS çš„çŸ­å¥åˆ—è¡¨ã€‚

    1. å…ˆæŒ‰å¥æœ«æ ‡ç‚¹ï¼ˆã€‚ï¼ï¼Ÿï¼›!?;\nï¼‰æ‹†åˆ†ã€‚
    2. è‹¥æŸæ®µä»è¶…è¿‡ max_lenï¼Œåˆ™æŒ‰æ¬¡çº§æ ‡ç‚¹ï¼ˆï¼Œ,ã€ï¼š:â€”ï¼‰ç»§ç»­æ‹†åˆ†ã€‚
    3. è‹¥ä»è¶…è¿‡ max_lenï¼Œåˆ™å¯¹åŠåˆ‡å‰²ï¼Œç›´åˆ°æ¯æ®µ <= max_lenã€‚
    """
    if not text or not text.strip():
        return []

    # ç¬¬ä¸€è½®ï¼šæŒ‰ä¸»è¦å¥æœ«æ ‡ç‚¹æ‹†åˆ†
    chunks = _PUNCT_PATTERN.split(text)
    chunks = [c.strip() for c in chunks if c.strip()]

    # ç¬¬äºŒè½®ï¼šå¯¹è¶…é•¿æ®µæŒ‰æ¬¡çº§æ ‡ç‚¹æ‹†åˆ†
    result = []
    for chunk in chunks:
        if len(chunk) <= max_len:
            result.append(chunk)
        else:
            sub_chunks = _SUB_PUNCT_PATTERN.split(chunk)
            sub_chunks = [s.strip() for s in sub_chunks if s.strip()]
            for sc in sub_chunks:
                if len(sc) <= max_len:
                    result.append(sc)
                else:
                    # é€’å½’å¯¹åŠæ‹†åˆ†
                    result.extend(_force_split(sc, max_len))
    return result

def _force_split(text: str, max_len: int) -> list[str]:
    """æ— åˆé€‚æ ‡ç‚¹æ—¶ï¼Œå¯¹åŠæ‹†åˆ†ç›´åˆ°æ¯æ®µ <= max_len"""
    if len(text) <= max_len:
        return [text]
    mid = len(text) // 2
    # å°½é‡åœ¨ä¸­é—´é™„è¿‘çš„ç©ºæ ¼æˆ–æ ‡ç‚¹å¤„åˆ‡å‰²
    best = mid
    for offset in range(min(20, mid)):
        for pos in (mid + offset, mid - offset):
            if 0 < pos < len(text) and text[pos] in ' ï¼Œ,ã€‚ï¼ï¼Ÿï¼›ã€ï¼š!?; \n':
                best = pos + 1
                break
        else:
            continue
        break
    left = text[:best].strip()
    right = text[best:].strip()
    parts = []
    if left:
        parts.extend(_force_split(left, max_len))
    if right:
        parts.extend(_force_split(right, max_len))
    return parts

class Communicator(QObject):
    trigger_show = pyqtSignal()
    append_chat = pyqtSignal(str, str) # å‘é€è€…, å†…å®¹
    request_exit = pyqtSignal()
    voice_status = pyqtSignal(str)  # è¯­éŸ³çŠ¶æ€æç¤º

class AIAssistant(QWidget):
    def __init__(self):
        super().__init__()
        self.comm = Communicator()
        self.client = ollama.Client(host=REMOTE_OLLAMA_HOST)
        self.model_name = MODEL_NAME
        
        # --- å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç† ---
        self.chat_history = []

        # --- è¯­éŸ³å½•åˆ¶çŠ¶æ€ ---
        self._recording = False
        self._recorded_frames = []

        # --- ASR / TTS æ¨¡å‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰ ---
        self.asr_model = None
        self.tts_model = None
        self.kokoro_model = None
        self.kokoro_pipeline = None
        self._models_loaded = False
        self._models_loading = False

        # --- MCP é…ç½® ---
        self.server_params = StdioServerParameters(
            command="python",
            args=["local_tools.py"], # ç¡®ä¿è·¯å¾„æ­£ç¡®
        )

        # --- UI åˆå§‹åŒ– ---
        self.init_ui()

        # åˆå§‹æ—¶å°è¯•åŒæ­¥ä¸€æ¬¡å·¥å…·åˆ—è¡¨
        self.sync_tools_from_mcp()
        
        # --- ä¿¡å·ç»‘å®š ---
        self.comm.trigger_show.connect(self.show_and_focus)
        self.comm.append_chat.connect(self.update_chat_display)
        self.comm.request_exit.connect(self.handle_exit)
        self.comm.voice_status.connect(lambda msg: self.update_chat_display("System", msg))

        # MCP å·¥å…·å®šä¹‰ï¼šç”± local_tools.py æä¾›ï¼Œè¿è¡Œæ—¶é€šè¿‡ sync_tools_from_mcp() åŠ¨æ€è·å–ã€‚
        # åˆå§‹ç•™ç©ºï¼Œè‹¥åŒæ­¥å¤±è´¥ä¼šå›é€€ä¸ºæœ€å°çš„ `run_command` å·¥å…·ã€‚
        self.tools = []

        # --- åˆå§‹åŒ– ASR å’Œ TTS æ¨¡å‹ ---
        print("æ­£åœ¨åå°åŠ è½½ ASR å’Œ TTS æ¨¡å‹...")
        self._load_voice_models()
        
        print("AI Assistant åˆå§‹åŒ–å®Œæˆã€‚")
    
    def sync_tools_from_mcp(self):
        """ä» MCP Server åŠ¨æ€è·å–å·¥å…·å®šä¹‰ï¼ŒåŒæ­¥ç»™ Ollama"""
        async def fetch():
            async with stdio_client(self.server_params) as (read, write):
                async with ClientSession(read, write) as session:
                    try:
                        await session.initialize()
                        tools = await session.list_tools()
                        # å°† MCP çš„å·¥å…·æ ¼å¼è½¬æ¢ä¸º Ollama éœ€è¦çš„æ ¼å¼
                        self.tools = []
                        for t in tools.tools:
                            self.tools.append({
                                'type': 'function',
                                'function': {
                                    'name': t.name,
                                    'description': t.description,
                                    'parameters': t.inputSchema
                                }
                            })
                        print(f"æˆåŠŸåŒæ­¥å·¥å…·: {[t['function']['name'] for t in self.tools]}")
                    except Exception as e:
                        print(f"åŒæ­¥å·¥å…·å¤±è´¥ï¼Œä½¿ç”¨å›é€€ run_commandï¼š{e}")
                        # è®¾ç½®æœ€å°å›é€€å·¥å…·ï¼ˆä¸ local_tools.py ä¸­çš„ run_command å¯¹åº”ï¼‰
                        self.tools = [{
                            'type': 'function',
                            'function': {
                                'name': 'run_command',
                                'description': 'åœ¨æœ¬åœ°ç”µè„‘æ‰§è¡Œç»ˆç«¯å‘½ä»¤',
                                'parameters': {
                                    'type': 'object',
                                    'properties': {
                                        'command': {'type': 'string', 'description': 'è¦æ‰§è¡Œçš„ CMD å‘½ä»¤'},
                                    },
                                    'required': ['command'],
                                },
                            },
                        }]

        threading.Thread(target=lambda: asyncio.run(fetch()), daemon=True).start()

    # --- æ ¸å¿ƒé€»è¾‘ï¼šè°ƒç”¨ MCP å·¥å…· ---
    async def call_mcp_tool(self, tool_name, arguments):
        """é€šè¿‡ MCP æ ‡å‡†æ¥å£è°ƒç”¨æœ¬åœ°å·¥å…·"""
        async with stdio_client(self.server_params) as (read, write):
            async with ClientSession(read, write) as session:
                await session.initialize()
                result = await session.call_tool(tool_name, arguments)
                # result.content é€šå¸¸æ˜¯ä¸€ä¸ª listï¼Œé‡Œé¢æœ‰ text å­—æ®µ
                return result.content[0].text if result.content else "No output"

    def init_ui(self):
        self.setWindowTitle("AI Research Assistant (Multi-turn)")
        self.setFixedSize(500, 600)
        # çª—å£ç½®é¡¶ï¼Œæ–¹ä¾¿éšæ—¶å”¤èµ·
        self.setWindowFlags(Qt.WindowType.WindowStaysOnTopHint)

        layout = QVBoxLayout()

        # 1. èŠå¤©è®°å½•æ˜¾ç¤ºåŒº
        self.display = QTextEdit()
        self.display.setReadOnly(True)
        layout.addWidget(QLabel("Chat History:"))
        layout.addWidget(self.display)

        # 2. è¾“å…¥åŒº
        self.input_field = QLineEdit()
        self.input_field.setPlaceholderText("è¾“å…¥æ¶ˆæ¯... (è¾“å…¥'ç»“æŸå¯¹è¯'æ¸…ç©ºè®°å½•)")
        self.input_field.returnPressed.connect(self.handle_send)
        
        # 3. æŒ‰é’®åŒº
        btn_layout = QHBoxLayout()
        self.send_btn = QPushButton("å‘é€")
        self.send_btn.clicked.connect(self.handle_send)
        self.clear_btn = QPushButton("ç»“æŸå½“å‰å¯¹è¯")
        self.clear_btn.clicked.connect(self.reset_chat)
        self.md_checkbox = QCheckBox("æ¸²æŸ“ Markdown")
        self.md_checkbox.setChecked(True)
        
        btn_layout.addWidget(self.send_btn)
        btn_layout.addWidget(self.clear_btn)
        btn_layout.addWidget(self.md_checkbox)
        
        layout.addWidget(self.input_field)
        layout.addLayout(btn_layout)
        
        self.setLayout(layout)

    # --- é€»è¾‘å¤„ç† ---
    def show_and_focus(self):
        self.show()
        self.activateWindow()
        self.input_field.setFocus()

    def update_chat_display(self, sender, content):
        # æ‹¦æˆªæ¥è‡ª Web ç«¯çš„æ¸…ç©ºä¿¡å·
        if sender == "__CLEAR__":
            self.reset_chat()
            return

        color = "#2c3e50" if sender == "AI" else "#2980b9"
        # æ ¹æ®å¤é€‰æ¡†å†³å®šæ˜¯å¦æ¸²æŸ“ Markdown
        if getattr(self, 'md_checkbox', None) and self.md_checkbox.isChecked():
            doc = QTextDocument()
            doc.setMarkdown(content)
            content_html = doc.toHtml()
        else:
            content_html = html.escape(content).replace('\n', '<br>')
        self.display.append(f"<b style='color:{color}'>{sender}:</b> {content_html}<br>")
        # è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
        self.display.verticalScrollBar().setValue(self.display.verticalScrollBar().maximum())

    def reset_chat(self):
        self.chat_history = []
        self.display.clear()
        self.update_chat_display("System", "å¯¹è¯ä¸Šä¸‹æ–‡å·²æ¸…ç©ºã€‚")

    def handle_send(self):
        user_text = self.input_field.text().strip()
        if not user_text:
            return
        
        if user_text in ["ç»“æŸå¯¹è¯", "exit", "clear", "quit"]:
            self.reset_chat()
            self.input_field.clear()
            return

        self.update_chat_display("Me", user_text)
        self.input_field.clear()
        self.input_field.setEnabled(False) # é˜²æ­¢é‡å¤å‘é€
        
        # å¼€å¯åå°çº¿ç¨‹å¤„ç† AI é€»è¾‘
        threading.Thread(target=self.process_ai_logic, args=(user_text,), daemon=True).start()

    def process_ai_logic(self, user_input, from_web=False):
        try:
            # åŠ å…¥ä¸Šä¸‹æ–‡
            self.chat_history.append({'role': 'user', 'content': user_input})

            # å¦‚æœæ¥è‡ª PyQt ç«¯ï¼ŒåŒæ­¥ç”¨æˆ·æ¶ˆæ¯åˆ° Web
            if not from_web:
                try:
                    web_broadcast("Me", user_input)
                except Exception:
                    pass
            
            # 1. ç¬¬ä¸€è½®è¯·æ±‚ (å« Tool è°ƒç”¨åˆ¤æ–­)
            response = self.client.chat(
                model=MODEL_NAME,
                messages=self.chat_history,
                tools=self.tools,
                keep_alive=-1
            )

            message = response.get('message', {})
            
            # 2. å¤„ç†å·¥å…·é“¾å¼è°ƒç”¨
            if message.get('tool_calls'):
                self.chat_history.append(message) # è®°å½•æ¨¡å‹çš„ tool_call è¯·æ±‚
                
                for tool_call in message['tool_calls']:
                    t_name = tool_call['function']['name']
                    t_args = tool_call['function']['arguments']

                    print(f"[MCP Action] æ­£åœ¨è°ƒç”¨å·¥å…·: {t_name} å‚æ•°: {t_args}")

                    output = asyncio.run(self.call_mcp_tool(t_name, t_args))

                    self.chat_history.append({
                        'role': 'tool', 
                        'content': str(output), 
                        'name': t_name
                    })

                # 3. å†æ¬¡è¯·æ±‚è·å–æœ€ç»ˆå›å¤
                final_response = self.client.chat(model=MODEL_NAME, messages=self.chat_history)
                final_content = final_response['message']['content']
                self.chat_history.append(final_response['message'])
                self.comm.append_chat.emit("AI", final_content)
                # åŒæ­¥ AI å›å¤åˆ° Web
                if not from_web:
                    try:
                        web_broadcast("AI", final_content)
                    except Exception:
                        pass
            else:
                # æ™®é€šå¯¹è¯
                self.chat_history.append(message)
                ai_content = message.get('content', '')
                self.comm.append_chat.emit("AI", ai_content)
                # åŒæ­¥ AI å›å¤åˆ° Web
                if not from_web:
                    try:
                        web_broadcast("AI", ai_content)
                    except Exception:
                        pass

        except Exception as e:
            self.comm.append_chat.emit("System Error", str(e))
        finally:
            self.input_field.setEnabled(True)
            self.input_field.setFocus()

    # ==================== è¯­éŸ³äº¤äº’åŠŸèƒ½ ====================

    def _load_voice_models(self):
        """åå°åŠ è½½ ASR å’Œ TTS æ¨¡å‹ï¼ˆé¦–æ¬¡ä½¿ç”¨æ—¶è§¦å‘ï¼‰"""
        if self._models_loaded or self._models_loading:
            return
        self._models_loading = True
        self.comm.voice_status.emit("æ­£åœ¨åŠ è½½ ASR å’Œ TTS æ¨¡å‹ï¼Œè¯·ç¨å€™...")

        def _load():
            try:
                print("[Voice] å¼€å§‹åŠ è½½ ASR æ¨¡å‹...")
                self.asr_model = Qwen3ASRModel.from_pretrained(
                    ASR_MODEL_ID,
                    dtype=torch.bfloat16,
                    device_map="cuda:0",
                    max_inference_batch_size=32,
                    max_new_tokens=256,
                )
                print("[Voice] ASR æ¨¡å‹åŠ è½½å®Œæˆ")

                if TTS_ENGINE == "kokoro":
                    print("[Voice] å¼€å§‹åŠ è½½ Kokoro TTS æ¨¡å‹...")
                    device = 'cuda' if torch.cuda.is_available() else 'cpu'
                    self.kokoro_model = KModel(repo_id=KOKORO_REPO_ID).to(device).eval()
                    en_pipeline = KPipeline(lang_code='a', repo_id=KOKORO_REPO_ID, model=False)
                    def en_callable(text):
                        return next(en_pipeline(text)).phonemes
                    self.kokoro_pipeline = KPipeline(
                        lang_code='z', repo_id=KOKORO_REPO_ID,
                        model=self.kokoro_model, en_callable=en_callable,
                    )
                    print("[Voice] Kokoro TTS æ¨¡å‹åŠ è½½å®Œæˆ")
                else:
                    print("[Voice] å¼€å§‹åŠ è½½ Qwen TTS æ¨¡å‹...")
                    self.tts_model = Qwen3TTSModel.from_pretrained(
                        TTS_MODEL_ID,
                        device_map="cuda:0",
                        dtype=torch.bfloat16,
                    )
                    print("[Voice] Qwen TTS æ¨¡å‹åŠ è½½å®Œæˆ")

                self._models_loaded = True
                self.comm.voice_status.emit("ASR / TTS æ¨¡å‹åŠ è½½å®Œæ¯•ï¼Œå¯ä»¥ä½¿ç”¨è¯­éŸ³å¯¹è¯äº†ã€‚")
            except Exception as e:
                self.comm.voice_status.emit(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print(f"[Voice] æ¨¡å‹åŠ è½½å¼‚å¸¸: {e}")
            finally:
                self._models_loading = False

        threading.Thread(target=_load, daemon=True).start()

    def _on_voice_key_press(self):
        """Ctrl+Alt+A æŒ‰ä¸‹ â†’ å¼€å§‹å½•éŸ³"""
        if self._recording:
            return
        self._recording = True
        self._recorded_frames = []
        self.comm.voice_status.emit("ğŸ™ï¸ æ­£åœ¨å½•éŸ³... æ¾å¼€ Ctrl+Alt+A åœæ­¢")
        print("[Voice] å¼€å§‹å½•éŸ³")

        def _record_callback(indata, frames, time_info, status):
            if self._recording:
                self._recorded_frames.append(indata.copy())

        self._audio_stream = sd.InputStream(
            samplerate=RECORD_SAMPLE_RATE,
            channels=1,
            dtype='float32',
            callback=_record_callback,
        )
        self._audio_stream.start()

    def _on_voice_key_release(self):
        """Ctrl+Alt+A æ¾å¼€ â†’ åœæ­¢å½•éŸ³ï¼Œå¯åŠ¨ ASRâ†’LLMâ†’TTS æµæ°´çº¿"""
        if not self._recording:
            return
        self._recording = False
        print("[Voice] åœæ­¢å½•éŸ³")

        try:
            self._audio_stream.stop()
            self._audio_stream.close()
        except Exception:
            pass

        if not self._recorded_frames:
            self.comm.voice_status.emit("æœªæ£€æµ‹åˆ°éŸ³é¢‘è¾“å…¥ã€‚")
            return

        audio_data = np.concatenate(self._recorded_frames, axis=0).flatten()
        self._recorded_frames = []

        # åå°æ‰§è¡Œ ASR â†’ LLM â†’ TTS
        threading.Thread(target=self._voice_pipeline, args=(audio_data,), daemon=True).start()

    def _voice_pipeline(self, audio_data: np.ndarray):
        """è¯­éŸ³å¯¹è¯å…¨æµç¨‹: ASR â†’ LLM(Streaming) â†’ TTS â†’ æ’­æ”¾
        
        ä¸‰çº§æµæ°´çº¿æ¶æ„:
          Thread-1: LLM streamingï¼Œé‡åˆ°æ ‡ç‚¹å°±æ‹†å¥æ¨å…¥ sentence_queue
          Thread-2: ä» sentence_queue å–å¥å­ï¼Œåˆæˆ TTS éŸ³é¢‘æ¨å…¥ audio_chunk_queue
          Thread-3: ä» audio_chunk_queue å–éŸ³é¢‘å—ï¼ŒOutputStream å®æ—¶æ’­æ”¾
        """
        try:
            # --- 1) ASR: è¯­éŸ³è½¬æ–‡å­— ---
            self.comm.voice_status.emit("æ­£åœ¨è¯†åˆ«è¯­éŸ³...")
            tmp_wav = os.path.join(tempfile.gettempdir(), "_voice_input.wav")
            sf.write(tmp_wav, audio_data, RECORD_SAMPLE_RATE)

            results = self.asr_model.transcribe(
                audio=tmp_wav,
                language=None,
            )
            user_text = results[0].text.strip()
            detected_lang = results[0].language
            print(f"[Voice ASR] è¯­è¨€={detected_lang}, æ–‡å­—={user_text}")

            if not user_text:
                self.comm.voice_status.emit("æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³ã€‚")
                return

            # æ˜¾ç¤ºè¯†åˆ«ç»“æœ
            self.comm.append_chat.emit("Me ğŸ¤", user_text)

            # --- 2) LLM ---
            llm_input = user_text + "\nï¼ˆå›å¤ä¸­å°½é‡ä¸è¦å‡ºç°ç‰¹æ®Šç¬¦å·ï¼Œç”¨æ–‡å­—è¡¨è¿°ä¾¿äºæœ—è¯»ï¼‰"
            self.chat_history.append({'role': 'user', 'content': llm_input})
            try:
                web_broadcast("Me ğŸ¤", user_text)
            except Exception:
                pass

            # --- 2-b) LLM Streaming + 3) TTS æµå¼åˆæˆæ’­æ”¾ ---
            # ä¸‰çº§æµæ°´çº¿: LLM streaming â†’ sentence_queue â†’ TTS â†’ audio_chunk_queue â†’ æ’­æ”¾
            # é‡åˆ°æ ‡ç‚¹å°±æŠŠå·²ç´¯ç§¯æ–‡æœ¬å‘ç»™ TTSï¼Œæ— éœ€ç­‰ LLM ç”Ÿæˆå®Œæ¯•

            _split_punct = set('ã€‚ï¼ï¼Ÿï¼›!?;\nï¼Œ,ã€ï¼š:')

            sentence_queue = queue.Queue()          # LLM â†’ TTS
            audio_chunk_queue = queue.Queue(maxsize=64)  # TTS â†’ Player
            SENTINEL = None
            sr_holder = [None]
            sr_ready = threading.Event()
            full_content_holder = [""]              # æ”¶é›†å®Œæ•´å›å¤

            # ---------- Thread-1: LLM Streaming â†’ sentence_queue ----------
            def _flush_buffer(buf):
                """å°† buffer ä¸­çš„æ–‡æœ¬æ‹†å¥åæ¨å…¥ sentence_queueï¼Œè¿”å›ç©ºä¸²"""
                if buf.strip():
                    for s in split_sentences_for_tts(buf.strip(), TTS_TOKEN_MAX_NUM):
                        sentence_queue.put(s)
                        print(f"[LLM Stream] â†’ TTS: {s}")
                return ""

            def _stream_response(stream_iter):
                """ä» streaming iterator ä¸­è¯»å– deltaï¼Œé‡åˆ°æ ‡ç‚¹å°±æ‹†å¥æ¨å…¥é˜Ÿåˆ—ã€‚
                è¿”å› (full_content, tool_calls_list)"""
                buf = ""
                full = ""
                tc_list = []
                for chunk in stream_iter:
                    msg = chunk.get('message', {})
                    # æ”¶é›† tool_calls
                    if msg.get('tool_calls'):
                        tc_list.extend(msg['tool_calls'])
                    delta = msg.get('content', '')
                    if not delta:
                        continue
                    buf += delta
                    full += delta
                    # æ‰¾ buffer ä¸­æœ€åä¸€ä¸ªæ ‡ç‚¹ä½ç½®
                    last_punct = -1
                    for i, ch in enumerate(buf):
                        if ch in _split_punct:
                            last_punct = i
                    if last_punct >= 0:
                        sentence = buf[:last_punct + 1].strip()
                        buf = buf[last_punct + 1:]
                        if sentence:
                            for s in split_sentences_for_tts(sentence, TTS_TOKEN_MAX_NUM):
                                sentence_queue.put(s)
                                print(f"[LLM Stream] â†’ TTS: {s}")
                # å‰©ä½™ buffer
                buf = _flush_buffer(buf)
                return full, tc_list

            def llm_streaming_producer():
                try:
                    # ç¬¬ä¸€è½® streamingï¼ˆå« Tool è°ƒç”¨æ£€æµ‹ï¼‰
                    stream1 = self.client.chat(
                        model=MODEL_NAME,
                        messages=self.chat_history,
                        tools=self.tools,
                        stream=True,
                        keep_alive=-1,
                    )
                    content1, tool_calls = _stream_response(stream1)

                    if tool_calls:
                        # è®°å½•æ¨¡å‹çš„ tool_call è¯·æ±‚
                        self.chat_history.append({
                            'role': 'assistant',
                            'content': content1,
                            'tool_calls': tool_calls,
                        })
                        for tc in tool_calls:
                            t_name = tc['function']['name']
                            t_args = tc['function']['arguments']
                            print(f"[MCP Action] è°ƒç”¨å·¥å…·: {t_name} å‚æ•°: {t_args}")
                            output = asyncio.run(self.call_mcp_tool(t_name, t_args))
                            self.chat_history.append({
                                'role': 'tool', 'content': str(output), 'name': t_name
                            })

                        # ç¬¬äºŒè½® streamingï¼ˆè·å–æœ€ç»ˆå›å¤ï¼‰
                        stream2 = self.client.chat(
                            model=MODEL_NAME,
                            messages=self.chat_history,
                            stream=True,
                        )
                        content2, _ = _stream_response(stream2)
                        self.chat_history.append({'role': 'assistant', 'content': content2})
                        full_content_holder[0] = content2
                    else:
                        # æ™®é€šå¯¹è¯ï¼Œå†…å®¹å·²åœ¨ streaming è¿‡ç¨‹ä¸­æ¨å…¥é˜Ÿåˆ—
                        self.chat_history.append({'role': 'assistant', 'content': content1})
                        full_content_holder[0] = content1
                except Exception as e:
                    print(f"[LLM Stream] å¼‚å¸¸: {e}")
                finally:
                    sentence_queue.put(SENTINEL)  # é€šçŸ¥ TTS ç”Ÿäº§è€…ç»“æŸ

            # ---------- Thread-2: sentence_queue â†’ TTS â†’ audio_chunk_queue ----------
            def tts_producer():
                """ä» sentence_queue è¯»å–å¥å­ï¼Œåˆæˆ TTSï¼Œæ¨å…¥ audio_chunk_queue"""
                CHUNK_SAMPLES = 4800  # çº¦ 200ms @24kHz
                tts_lang = TTS_LANGUAGE
                i = 0
                while True:
                    sentence = sentence_queue.get()
                    if sentence is SENTINEL:
                        break
                    i += 1
                    try:
                        self.comm.voice_status.emit(f"æ­£åœ¨åˆæˆè¯­éŸ³ ({i})...")
                        if TTS_ENGINE == "kokoro":
                            def speed_callable(len_ps):
                                speed = 0.8
                                if len_ps <= 83:
                                    speed = 1
                                elif len_ps < 183:
                                    speed = 1 - (len_ps - 83) / 500
                                return speed * 1.5
                            generator = self.kokoro_pipeline(
                                sentence, voice=KOKORO_VOICE, speed=speed_callable,
                            )
                            result = next(generator)
                            wav = result.audio
                            if isinstance(wav, torch.Tensor):
                                wav = wav.cpu().numpy()
                            sr = KOKORO_SAMPLE_RATE
                        else:
                            wavs, sr = self.tts_model.generate_custom_voice(
                                text=sentence,
                                language=tts_lang,
                                speaker=TTS_SPEAKER,
                            )
                            wav = wavs[0]
                        # é¦–æ¬¡æ‹¿åˆ° sr åé€šçŸ¥æ’­æ”¾çº¿ç¨‹
                        if sr_holder[0] is None:
                            sr_holder[0] = sr
                            sr_ready.set()
                        # å°†æ•´æ®µéŸ³é¢‘åˆ‡æˆå°å—æ¨å…¥é˜Ÿåˆ—
                        offset = 0
                        while offset < len(wav):
                            chunk = wav[offset:offset + CHUNK_SAMPLES]
                            audio_chunk_queue.put(chunk)
                            offset += CHUNK_SAMPLES
                        print(f"[Voice TTS] åˆæˆå®Œæˆ ({i}): {sentence}")
                    except Exception as e:
                        print(f"[Voice TTS] åˆæˆç¬¬ {i} æ®µå¤±è´¥: {e}")
                audio_chunk_queue.put(SENTINEL)

            # ---------- Thread-3: audio_chunk_queue â†’ OutputStream æ’­æ”¾ ----------
            def audio_player():
                """ä½¿ç”¨ sd.OutputStream ä»é˜Ÿåˆ—æµå¼æ’­æ”¾éŸ³é¢‘"""
                sr_ready.wait()
                sr = sr_holder[0]
                PLAYBACK_BLOCK = 1024

                buffer = np.array([], dtype=np.float32)
                finished = False

                def callback(outdata, frames, time_info, status):
                    nonlocal buffer, finished
                    needed = frames
                    while len(buffer) < needed and not finished:
                        try:
                            chunk = audio_chunk_queue.get_nowait()
                            if chunk is SENTINEL:
                                finished = True
                                break
                            buffer = np.concatenate([buffer, chunk.astype(np.float32)])
                        except queue.Empty:
                            break

                    if len(buffer) >= needed:
                        outdata[:, 0] = buffer[:needed]
                        buffer = buffer[needed:]
                    else:
                        avail = len(buffer)
                        outdata[:avail, 0] = buffer[:avail]
                        outdata[avail:, 0] = 0.0
                        buffer = np.array([], dtype=np.float32)
                        if finished:
                            raise sd.CallbackStop()

                with sd.OutputStream(
                    samplerate=sr, channels=1, dtype='float32',
                    blocksize=PLAYBACK_BLOCK, callback=callback,
                ) as stream:
                    while stream.active:
                        if not finished:
                            try:
                                chunk = audio_chunk_queue.get(timeout=0.05)
                                if chunk is SENTINEL:
                                    finished = True
                                else:
                                    buffer = np.concatenate([buffer, chunk.astype(np.float32)])
                            except queue.Empty:
                                pass
                        else:
                            sd.sleep(50)
                print("[Voice TTS] OutputStream æ’­æ”¾ç»“æŸ")

            # å¯åŠ¨ä¸‰çº§æµæ°´çº¿
            llm_thread = threading.Thread(target=llm_streaming_producer, daemon=True)
            tts_thread = threading.Thread(target=tts_producer, daemon=True)
            player_thread = threading.Thread(target=audio_player, daemon=True)
            llm_thread.start()
            tts_thread.start()
            player_thread.start()

            llm_thread.join()
            # LLM å®Œæ¯•ï¼Œæ›´æ–° UI å’Œ Web
            ai_content = full_content_holder[0]
            if ai_content:
                self.comm.append_chat.emit("AI", ai_content)
                try:
                    web_broadcast("AI", ai_content)
                except Exception:
                    pass

            tts_thread.join()
            player_thread.join()
            self.comm.voice_status.emit("è¯­éŸ³æ’­æ”¾å®Œæ¯•ã€‚")

        except Exception as e:
            self.comm.voice_status.emit(f"è¯­éŸ³å¤„ç†å¼‚å¸¸: {e}")
            print(f"[Voice] å¼‚å¸¸: {e}")

    # ==================== Web ç«¯è¯­éŸ³å¯¹è¯ ====================

    def web_voice_pipeline(self, audio_bytes, emit_fn):
        """Web ç«¯è¯­éŸ³å¯¹è¯å…¨æµç¨‹: ASR â†’ LLM(Streaming) â†’ TTS â†’ æµå¼æ¨é€éŸ³é¢‘åˆ°æµè§ˆå™¨

        emit_fn(event, data): å‘æŒ‡å®š Web å®¢æˆ·ç«¯å‘é€ Socket.IO äº‹ä»¶
        Events:
            voice_status:      {"status": str, "message": str}
            voice_asr_result:  {"text": str}
            voice_audio_start: {"sampleRate": int}
            voice_audio_chunk: bytes (PCM float32)
            voice_audio_end:   {}
        """
        try:
            if not self._models_loaded:
                emit_fn("voice_status", {"status": "error", "message": "è¯­éŸ³æ¨¡å‹å°šæœªåŠ è½½å®Œæˆï¼Œè¯·ç¨åå†è¯•"})
                return

            # --- 1) è§£ç  PCM â†’ ASR ---
            audio_data = np.frombuffer(audio_bytes, dtype=np.float32)
            if len(audio_data) < RECORD_SAMPLE_RATE * 0.3:  # ä¸è¶³ 0.3 ç§’
                emit_fn("voice_status", {"status": "done", "message": "å½•éŸ³æ—¶é—´å¤ªçŸ­"})
                return

            emit_fn("voice_status", {"status": "asr", "message": "æ­£åœ¨è¯†åˆ«è¯­éŸ³..."})

            tmp_wav = os.path.join(tempfile.gettempdir(), "_web_voice_input.wav")
            sf.write(tmp_wav, audio_data, RECORD_SAMPLE_RATE)

            results = self.asr_model.transcribe(audio=tmp_wav, language=None)
            user_text = results[0].text.strip()
            print(f"[Web Voice ASR] æ–‡å­—={user_text}")

            if not user_text:
                emit_fn("voice_status", {"status": "done", "message": "æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³"})
                return

            emit_fn("voice_asr_result", {"text": user_text})

            # å¹¿æ’­ç”¨æˆ·æ¶ˆæ¯åˆ° PyQt å’Œ Web
            self.comm.append_chat.emit("Me ğŸ¤", user_text)
            try:
                web_broadcast("Me ğŸ¤", user_text)
            except Exception:
                pass

            # --- 2) LLM Streaming + TTS â†’ æµå¼æ¨é€éŸ³é¢‘ ---
            emit_fn("voice_status", {"status": "llm", "message": "AI æ€è€ƒä¸­..."})

            llm_input = user_text + "\nï¼ˆå›å¤ä¸­å°½é‡ä¸è¦å‡ºç°ç‰¹æ®Šç¬¦å·ï¼Œç”¨æ–‡å­—è¡¨è¿°ä¾¿äºæœ—è¯»ï¼‰"
            self.chat_history.append({'role': 'user', 'content': llm_input})

            _split_punct = set('ã€‚ï¼ï¼Ÿï¼›!?;\nï¼Œ,ã€ï¼š:')
            sentence_queue = queue.Queue()
            SENTINEL = None
            full_content_holder = [""]
            sr_sent = [False]

            # --- Thread-1: LLM Streaming â†’ sentence_queue ---
            def _flush_buffer(buf):
                if buf.strip():
                    for s in split_sentences_for_tts(buf.strip(), TTS_TOKEN_MAX_NUM):
                        sentence_queue.put(s)
                        print(f"[Web LLM Stream] â†’ TTS: {s}")
                return ""

            def _stream_response(stream_iter):
                buf = ""
                full = ""
                tc_list = []
                for chunk in stream_iter:
                    msg = chunk.get('message', {})
                    if msg.get('tool_calls'):
                        tc_list.extend(msg['tool_calls'])
                    delta = msg.get('content', '')
                    if not delta:
                        continue
                    buf += delta
                    full += delta
                    last_punct = -1
                    for i, ch in enumerate(buf):
                        if ch in _split_punct:
                            last_punct = i
                    if last_punct >= 0:
                        sentence = buf[:last_punct + 1].strip()
                        buf = buf[last_punct + 1:]
                        if sentence:
                            for s in split_sentences_for_tts(sentence, TTS_TOKEN_MAX_NUM):
                                sentence_queue.put(s)
                                print(f"[Web LLM Stream] â†’ TTS: {s}")
                buf = _flush_buffer(buf)
                return full, tc_list

            def llm_streaming_producer():
                try:
                    stream1 = self.client.chat(
                        model=MODEL_NAME,
                        messages=self.chat_history,
                        tools=self.tools,
                        stream=True,
                        keep_alive=-1,
                    )
                    content1, tool_calls = _stream_response(stream1)

                    if tool_calls:
                        self.chat_history.append({
                            'role': 'assistant',
                            'content': content1,
                            'tool_calls': tool_calls,
                        })
                        for tc in tool_calls:
                            t_name = tc['function']['name']
                            t_args = tc['function']['arguments']
                            print(f"[Web MCP Action] è°ƒç”¨å·¥å…·: {t_name} å‚æ•°: {t_args}")
                            output = asyncio.run(self.call_mcp_tool(t_name, t_args))
                            self.chat_history.append({
                                'role': 'tool', 'content': str(output), 'name': t_name
                            })
                        stream2 = self.client.chat(
                            model=MODEL_NAME,
                            messages=self.chat_history,
                            stream=True,
                        )
                        content2, _ = _stream_response(stream2)
                        self.chat_history.append({'role': 'assistant', 'content': content2})
                        full_content_holder[0] = content2
                    else:
                        self.chat_history.append({'role': 'assistant', 'content': content1})
                        full_content_holder[0] = content1
                except Exception as e:
                    print(f"[Web LLM Stream] å¼‚å¸¸: {e}")
                finally:
                    sentence_queue.put(SENTINEL)

            # --- Thread-2: sentence_queue â†’ TTS â†’ emit audio chunks ---
            def tts_web_producer():
                i = 0
                while True:
                    sentence = sentence_queue.get()
                    if sentence is SENTINEL:
                        break
                    i += 1
                    try:
                        emit_fn("voice_status", {"status": "tts", "message": f"æ­£åœ¨åˆæˆè¯­éŸ³ ({i})..."})

                        if TTS_ENGINE == "kokoro":
                            def speed_callable(len_ps):
                                speed = 0.8
                                if len_ps <= 83:
                                    speed = 1
                                elif len_ps < 183:
                                    speed = 1 - (len_ps - 83) / 500
                                return speed * 1.5

                            generator = self.kokoro_pipeline(
                                sentence, voice=KOKORO_VOICE, speed=speed_callable,
                            )
                            result = next(generator)
                            wav = result.audio
                            if isinstance(wav, torch.Tensor):
                                wav = wav.cpu().numpy()
                            sr = KOKORO_SAMPLE_RATE
                        else:
                            wavs, sr = self.tts_model.generate_custom_voice(
                                text=sentence,
                                language=TTS_LANGUAGE,
                                speaker=TTS_SPEAKER,
                            )
                            wav = wavs[0]

                        # é¦–æ¬¡å‘é€é‡‡æ ·ç‡
                        if not sr_sent[0]:
                            emit_fn("voice_audio_start", {"sampleRate": sr})
                            sr_sent[0] = True

                        # å‘é€ PCM éŸ³é¢‘æ•°æ®
                        wav_f32 = wav.astype(np.float32)
                        emit_fn("voice_audio_chunk", wav_f32.tobytes())
                        print(f"[Web Voice TTS] åˆæˆå®Œæˆ ({i}): {sentence[:30]}...")

                    except Exception as e:
                        print(f"[Web Voice TTS] åˆæˆç¬¬ {i} æ®µå¤±è´¥: {e}")

                emit_fn("voice_audio_end", {})

            # å¯åŠ¨ä¸¤çº§æµæ°´çº¿
            llm_thread = threading.Thread(target=llm_streaming_producer, daemon=True)
            tts_thread = threading.Thread(target=tts_web_producer, daemon=True)
            llm_thread.start()
            tts_thread.start()

            llm_thread.join()
            # LLM å®Œæ¯•ï¼Œæ›´æ–° UI
            ai_content = full_content_holder[0]
            if ai_content:
                self.comm.append_chat.emit("AI", ai_content)
                try:
                    web_broadcast("AI", ai_content)
                except Exception:
                    pass

            tts_thread.join()

        except Exception as e:
            emit_fn("voice_status", {"status": "error", "message": f"è¯­éŸ³å¤„ç†å¼‚å¸¸: {e}"})
            print(f"[Web Voice] å¼‚å¸¸: {e}")

    def handle_exit(self):
        print("åŠ©æ‰‹æ­£åœ¨é€€å‡º...")
        QApplication.quit()

    def run_hotkey_listener(self):
        keyboard.add_hotkey('ctrl+alt+q', lambda: self.comm.trigger_show.emit())
        keyboard.add_hotkey('ctrl+alt+e', lambda: self.comm.request_exit.emit())
        # è¯­éŸ³å¿«æ·é”®ï¼šæŒ‰ä¸‹å¼€å§‹å½•éŸ³ï¼Œæ¾å¼€åœæ­¢
        keyboard.on_press_key('a', lambda e: self._on_voice_key_press() if keyboard.is_pressed('ctrl') and keyboard.is_pressed('alt') else None)
        keyboard.on_release_key('a', lambda e: self._on_voice_key_release() if not keyboard.is_pressed('a') else None)
        print("åŠ©æ‰‹å·²å¯åŠ¨ (Ctrl+Alt+Q å”¤èµ·, Ctrl+Alt+E é€€å‡º, Ctrl+Alt+A è¯­éŸ³å¯¹è¯)")

if __name__ == "__main__":
    app = QApplication(sys.argv)
    assistant = AIAssistant()
    assistant.run_hotkey_listener()

    # å¯åŠ¨ Web Chat æœåŠ¡ï¼ˆå±€åŸŸç½‘å¯è®¿é—®ï¼‰
    set_assistant(assistant)
    start_web_server(host="0.0.0.0", port=5100)

    sys.exit(app.exec())
