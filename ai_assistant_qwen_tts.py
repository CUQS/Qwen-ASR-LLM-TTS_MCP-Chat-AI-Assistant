import sys
import os
import re
import threading
import queue
import keyboard
import ollama
import subprocess
import asyncio  # MCP æ˜¯å¼‚æ­¥çš„
import tempfile
import numpy as np
import sounddevice as sd
import soundfile as sf
import torch
from PyQt6.QtWidgets import (QApplication, QWidget, QVBoxLayout, QTextEdit, 
                             QLineEdit, QPushButton, QHBoxLayout, QLabel, QCheckBox)
from PyQt6.QtCore import QObject, pyqtSignal, Qt
from PyQt6.QtGui import QTextDocument
import html

# --- ASR / TTS ---
from qwen_asr import Qwen3ASRModel
from qwen_tts import Qwen3TTSModel

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# --- Web Chat é›†æˆ ---
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'webpage_chat'))
from server import set_assistant, start_server as start_web_server, broadcast_message as web_broadcast

# --- é…ç½®åŒº ---
REMOTE_OLLAMA_HOST = "http://192.168.40.12:11434" 
MODEL_NAME = "dengcao/Qwen3-30B-A3B-Instruct-2507"

# ASR / TTS æ¨¡å‹é…ç½®
ASR_MODEL_ID = "Qwen/Qwen3-ASR-0.6B"
TTS_MODEL_ID = "Qwen/Qwen3-TTS-12Hz-0.6B-CustomVoice"
TTS_SPEAKER = "Serena"
TTS_LANGUAGE = "Chinese"
TTS_TOKEN_MAX_NUM = 30  # TTS å•å¥æœ€å¤§å­—ç¬¦æ•°ï¼Œè¶…è¿‡åˆ™ç»§ç»­æ‹†åˆ†
RECORD_SAMPLE_RATE = 16000  # ASR è¦æ±‚ 16kHz

# --- å¥å­æ‹†åˆ†å·¥å…· ---
_PUNCT_PATTERN = re.compile(r'(?<=[ã€‚ï¼ï¼Ÿï¼›\n!\?;])')
_SUB_PUNCT_PATTERN = re.compile(r'(?<=[ï¼Œ,ã€ï¼š:\-â€”])')

def split_sentences_for_tts(text: str, max_len: int = TTS_TOKEN_MAX_NUM) -> list[str]:
    """æŒ‰æ ‡ç‚¹å°†æ–‡æœ¬æ‹†åˆ†ä¸ºé€‚åˆ TTS çš„çŸ­å¥åˆ—è¡¨ã€‚

    1. å…ˆæŒ‰å¥æœ«æ ‡ç‚¹ï¼ˆã€‚ï¼ï¼Ÿï¼›!?;\nï¼‰æ‹†åˆ†ã€‚
    2. è‹¥æŸæ®µä»è¶…è¿‡ max_lenï¼Œåˆ™æŒ‰æ¬¡çº§æ ‡ç‚¹ï¼ˆï¼Œ,ã€ï¼š:â€”ï¼‰ç»§ç»­æ‹†åˆ†ã€‚
    3. è‹¥ä»è¶…è¿‡ max_lenï¼Œåˆ™å¯¹åŠåˆ‡å‰²ï¼Œç›´åˆ°æ¯æ®µ <= max_lenã€‚
    """
    if not text or not text.strip():
        return []

    # ç¬¬ä¸€è½®ï¼šæŒ‰ä¸»è¦å¥æœ«æ ‡ç‚¹æ‹†åˆ†
    chunks = _PUNCT_PATTERN.split(text)
    chunks = [c.strip() for c in chunks if c.strip()]

    # ç¬¬äºŒè½®ï¼šå¯¹è¶…é•¿æ®µæŒ‰æ¬¡çº§æ ‡ç‚¹æ‹†åˆ†
    result = []
    for chunk in chunks:
        if len(chunk) <= max_len:
            result.append(chunk)
        else:
            sub_chunks = _SUB_PUNCT_PATTERN.split(chunk)
            sub_chunks = [s.strip() for s in sub_chunks if s.strip()]
            for sc in sub_chunks:
                if len(sc) <= max_len:
                    result.append(sc)
                else:
                    # é€’å½’å¯¹åŠæ‹†åˆ†
                    result.extend(_force_split(sc, max_len))
    return result

def _force_split(text: str, max_len: int) -> list[str]:
    """æ— åˆé€‚æ ‡ç‚¹æ—¶ï¼Œå¯¹åŠæ‹†åˆ†ç›´åˆ°æ¯æ®µ <= max_len"""
    if len(text) <= max_len:
        return [text]
    mid = len(text) // 2
    # å°½é‡åœ¨ä¸­é—´é™„è¿‘çš„ç©ºæ ¼æˆ–æ ‡ç‚¹å¤„åˆ‡å‰²
    best = mid
    for offset in range(min(20, mid)):
        for pos in (mid + offset, mid - offset):
            if 0 < pos < len(text) and text[pos] in ' ï¼Œ,ã€‚ï¼ï¼Ÿï¼›ã€ï¼š!?; \n':
                best = pos + 1
                break
        else:
            continue
        break
    left = text[:best].strip()
    right = text[best:].strip()
    parts = []
    if left:
        parts.extend(_force_split(left, max_len))
    if right:
        parts.extend(_force_split(right, max_len))
    return parts

class Communicator(QObject):
    trigger_show = pyqtSignal()
    append_chat = pyqtSignal(str, str) # å‘é€è€…, å†…å®¹
    request_exit = pyqtSignal()
    voice_status = pyqtSignal(str)  # è¯­éŸ³çŠ¶æ€æç¤º

class AIAssistant(QWidget):
    def __init__(self):
        super().__init__()
        self.comm = Communicator()
        self.client = ollama.Client(host=REMOTE_OLLAMA_HOST)
        self.model_name = MODEL_NAME
        
        # --- å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç† ---
        self.chat_history = []

        # --- è¯­éŸ³å½•åˆ¶çŠ¶æ€ ---
        self._recording = False
        self._recorded_frames = []

        # --- ASR / TTS æ¨¡å‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰ ---
        self.asr_model = None
        self.tts_model = None
        self._models_loaded = False
        self._models_loading = False

        # --- MCP é…ç½® ---
        self.server_params = StdioServerParameters(
            command="python",
            args=["local_tools.py"], # ç¡®ä¿è·¯å¾„æ­£ç¡®
        )

        # --- UI åˆå§‹åŒ– ---
        self.init_ui()

        # åˆå§‹æ—¶å°è¯•åŒæ­¥ä¸€æ¬¡å·¥å…·åˆ—è¡¨
        self.sync_tools_from_mcp()
        
        # --- ä¿¡å·ç»‘å®š ---
        self.comm.trigger_show.connect(self.show_and_focus)
        self.comm.append_chat.connect(self.update_chat_display)
        self.comm.request_exit.connect(self.handle_exit)
        self.comm.voice_status.connect(lambda msg: self.update_chat_display("System", msg))

        # MCP å·¥å…·å®šä¹‰ï¼šç”± local_tools.py æä¾›ï¼Œè¿è¡Œæ—¶é€šè¿‡ sync_tools_from_mcp() åŠ¨æ€è·å–ã€‚
        # åˆå§‹ç•™ç©ºï¼Œè‹¥åŒæ­¥å¤±è´¥ä¼šå›é€€ä¸ºæœ€å°çš„ `run_command` å·¥å…·ã€‚
        self.tools = []

        # --- åˆå§‹åŒ– ASR å’Œ TTS æ¨¡å‹ ---
        print("æ­£åœ¨åå°åŠ è½½ ASR å’Œ TTS æ¨¡å‹...")
        self._load_voice_models()
        
        print("AI Assistant åˆå§‹åŒ–å®Œæˆã€‚")
    
    def sync_tools_from_mcp(self):
        """ä» MCP Server åŠ¨æ€è·å–å·¥å…·å®šä¹‰ï¼ŒåŒæ­¥ç»™ Ollama"""
        async def fetch():
            async with stdio_client(self.server_params) as (read, write):
                async with ClientSession(read, write) as session:
                    try:
                        await session.initialize()
                        tools = await session.list_tools()
                        # å°† MCP çš„å·¥å…·æ ¼å¼è½¬æ¢ä¸º Ollama éœ€è¦çš„æ ¼å¼
                        self.tools = []
                        for t in tools.tools:
                            self.tools.append({
                                'type': 'function',
                                'function': {
                                    'name': t.name,
                                    'description': t.description,
                                    'parameters': t.inputSchema
                                }
                            })
                        print(f"æˆåŠŸåŒæ­¥å·¥å…·: {[t['function']['name'] for t in self.tools]}")
                    except Exception as e:
                        print(f"åŒæ­¥å·¥å…·å¤±è´¥ï¼Œä½¿ç”¨å›é€€ run_commandï¼š{e}")
                        # è®¾ç½®æœ€å°å›é€€å·¥å…·ï¼ˆä¸ local_tools.py ä¸­çš„ run_command å¯¹åº”ï¼‰
                        self.tools = [{
                            'type': 'function',
                            'function': {
                                'name': 'run_command',
                                'description': 'åœ¨æœ¬åœ°ç”µè„‘æ‰§è¡Œç»ˆç«¯å‘½ä»¤',
                                'parameters': {
                                    'type': 'object',
                                    'properties': {
                                        'command': {'type': 'string', 'description': 'è¦æ‰§è¡Œçš„ CMD å‘½ä»¤'},
                                    },
                                    'required': ['command'],
                                },
                            },
                        }]

        threading.Thread(target=lambda: asyncio.run(fetch()), daemon=True).start()

    # --- æ ¸å¿ƒé€»è¾‘ï¼šè°ƒç”¨ MCP å·¥å…· ---
    async def call_mcp_tool(self, tool_name, arguments):
        """é€šè¿‡ MCP æ ‡å‡†æ¥å£è°ƒç”¨æœ¬åœ°å·¥å…·"""
        async with stdio_client(self.server_params) as (read, write):
            async with ClientSession(read, write) as session:
                await session.initialize()
                result = await session.call_tool(tool_name, arguments)
                # result.content é€šå¸¸æ˜¯ä¸€ä¸ª listï¼Œé‡Œé¢æœ‰ text å­—æ®µ
                return result.content[0].text if result.content else "No output"

    def init_ui(self):
        self.setWindowTitle("AI Research Assistant (Multi-turn)")
        self.setFixedSize(500, 600)
        # çª—å£ç½®é¡¶ï¼Œæ–¹ä¾¿éšæ—¶å”¤èµ·
        self.setWindowFlags(Qt.WindowType.WindowStaysOnTopHint)

        layout = QVBoxLayout()

        # 1. èŠå¤©è®°å½•æ˜¾ç¤ºåŒº
        self.display = QTextEdit()
        self.display.setReadOnly(True)
        layout.addWidget(QLabel("Chat History:"))
        layout.addWidget(self.display)

        # 2. è¾“å…¥åŒº
        self.input_field = QLineEdit()
        self.input_field.setPlaceholderText("è¾“å…¥æ¶ˆæ¯... (è¾“å…¥'ç»“æŸå¯¹è¯'æ¸…ç©ºè®°å½•)")
        self.input_field.returnPressed.connect(self.handle_send)
        
        # 3. æŒ‰é’®åŒº
        btn_layout = QHBoxLayout()
        self.send_btn = QPushButton("å‘é€")
        self.send_btn.clicked.connect(self.handle_send)
        self.clear_btn = QPushButton("ç»“æŸå½“å‰å¯¹è¯")
        self.clear_btn.clicked.connect(self.reset_chat)
        self.md_checkbox = QCheckBox("æ¸²æŸ“ Markdown")
        self.md_checkbox.setChecked(True)
        
        btn_layout.addWidget(self.send_btn)
        btn_layout.addWidget(self.clear_btn)
        btn_layout.addWidget(self.md_checkbox)
        
        layout.addWidget(self.input_field)
        layout.addLayout(btn_layout)
        
        self.setLayout(layout)

    # --- é€»è¾‘å¤„ç† ---
    def show_and_focus(self):
        self.show()
        self.activateWindow()
        self.input_field.setFocus()

    def update_chat_display(self, sender, content):
        # æ‹¦æˆªæ¥è‡ª Web ç«¯çš„æ¸…ç©ºä¿¡å·
        if sender == "__CLEAR__":
            self.reset_chat()
            return

        color = "#2c3e50" if sender == "AI" else "#2980b9"
        # æ ¹æ®å¤é€‰æ¡†å†³å®šæ˜¯å¦æ¸²æŸ“ Markdown
        if getattr(self, 'md_checkbox', None) and self.md_checkbox.isChecked():
            doc = QTextDocument()
            doc.setMarkdown(content)
            content_html = doc.toHtml()
        else:
            content_html = html.escape(content).replace('\n', '<br>')
        self.display.append(f"<b style='color:{color}'>{sender}:</b> {content_html}<br>")
        # è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
        self.display.verticalScrollBar().setValue(self.display.verticalScrollBar().maximum())

    def reset_chat(self):
        self.chat_history = []
        self.display.clear()
        self.update_chat_display("System", "å¯¹è¯ä¸Šä¸‹æ–‡å·²æ¸…ç©ºã€‚")

    def handle_send(self):
        user_text = self.input_field.text().strip()
        if not user_text:
            return
        
        if user_text in ["ç»“æŸå¯¹è¯", "exit", "clear", "quit"]:
            self.reset_chat()
            self.input_field.clear()
            return

        self.update_chat_display("Me", user_text)
        self.input_field.clear()
        self.input_field.setEnabled(False) # é˜²æ­¢é‡å¤å‘é€
        
        # å¼€å¯åå°çº¿ç¨‹å¤„ç† AI é€»è¾‘
        threading.Thread(target=self.process_ai_logic, args=(user_text,), daemon=True).start()

    def process_ai_logic(self, user_input, from_web=False):
        try:
            # åŠ å…¥ä¸Šä¸‹æ–‡
            self.chat_history.append({'role': 'user', 'content': user_input})

            # å¦‚æœæ¥è‡ª PyQt ç«¯ï¼ŒåŒæ­¥ç”¨æˆ·æ¶ˆæ¯åˆ° Web
            if not from_web:
                try:
                    web_broadcast("Me", user_input)
                except Exception:
                    pass
            
            # 1. ç¬¬ä¸€è½®è¯·æ±‚ (å« Tool è°ƒç”¨åˆ¤æ–­)
            response = self.client.chat(
                model=MODEL_NAME,
                messages=self.chat_history,
                tools=self.tools,
                keep_alive=-1
            )

            message = response.get('message', {})
            
            # 2. å¤„ç†å·¥å…·é“¾å¼è°ƒç”¨
            if message.get('tool_calls'):
                self.chat_history.append(message) # è®°å½•æ¨¡å‹çš„ tool_call è¯·æ±‚
                
                for tool_call in message['tool_calls']:
                    t_name = tool_call['function']['name']
                    t_args = tool_call['function']['arguments']

                    print(f"[MCP Action] æ­£åœ¨è°ƒç”¨å·¥å…·: {t_name} å‚æ•°: {t_args}")

                    output = asyncio.run(self.call_mcp_tool(t_name, t_args))

                    self.chat_history.append({
                        'role': 'tool', 
                        'content': str(output), 
                        'name': t_name
                    })

                # 3. å†æ¬¡è¯·æ±‚è·å–æœ€ç»ˆå›å¤
                final_response = self.client.chat(model=MODEL_NAME, messages=self.chat_history)
                final_content = final_response['message']['content']
                self.chat_history.append(final_response['message'])
                self.comm.append_chat.emit("AI", final_content)
                # åŒæ­¥ AI å›å¤åˆ° Web
                if not from_web:
                    try:
                        web_broadcast("AI", final_content)
                    except Exception:
                        pass
            else:
                # æ™®é€šå¯¹è¯
                self.chat_history.append(message)
                ai_content = message.get('content', '')
                self.comm.append_chat.emit("AI", ai_content)
                # åŒæ­¥ AI å›å¤åˆ° Web
                if not from_web:
                    try:
                        web_broadcast("AI", ai_content)
                    except Exception:
                        pass

        except Exception as e:
            self.comm.append_chat.emit("System Error", str(e))
        finally:
            self.input_field.setEnabled(True)
            self.input_field.setFocus()

    # ==================== è¯­éŸ³äº¤äº’åŠŸèƒ½ ====================

    def _load_voice_models(self):
        """åå°åŠ è½½ ASR å’Œ TTS æ¨¡å‹ï¼ˆé¦–æ¬¡ä½¿ç”¨æ—¶è§¦å‘ï¼‰"""
        if self._models_loaded or self._models_loading:
            return
        self._models_loading = True
        self.comm.voice_status.emit("æ­£åœ¨åŠ è½½ ASR å’Œ TTS æ¨¡å‹ï¼Œè¯·ç¨å€™...")

        def _load():
            try:
                print("[Voice] å¼€å§‹åŠ è½½ ASR æ¨¡å‹...")
                self.asr_model = Qwen3ASRModel.from_pretrained(
                    ASR_MODEL_ID,
                    dtype=torch.bfloat16,
                    device_map="cuda:0",
                    max_inference_batch_size=32,
                    max_new_tokens=256,
                )
                print("[Voice] ASR æ¨¡å‹åŠ è½½å®Œæˆ")

                print("[Voice] å¼€å§‹åŠ è½½ TTS æ¨¡å‹...")
                self.tts_model = Qwen3TTSModel.from_pretrained(
                    TTS_MODEL_ID,
                    device_map="cuda:0",
                    dtype=torch.bfloat16,
                )
                print("[Voice] TTS æ¨¡å‹åŠ è½½å®Œæˆ")

                self._models_loaded = True
                self.comm.voice_status.emit("ASR / TTS æ¨¡å‹åŠ è½½å®Œæ¯•ï¼Œå¯ä»¥ä½¿ç”¨è¯­éŸ³å¯¹è¯äº†ã€‚")
            except Exception as e:
                self.comm.voice_status.emit(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print(f"[Voice] æ¨¡å‹åŠ è½½å¼‚å¸¸: {e}")
            finally:
                self._models_loading = False

        threading.Thread(target=_load, daemon=True).start()

    def _on_voice_key_press(self):
        """Ctrl+Alt+A æŒ‰ä¸‹ â†’ å¼€å§‹å½•éŸ³"""
        if self._recording:
            return
        self._recording = True
        self._recorded_frames = []
        self.comm.voice_status.emit("ğŸ™ï¸ æ­£åœ¨å½•éŸ³... æ¾å¼€ Ctrl+Alt+A åœæ­¢")
        print("[Voice] å¼€å§‹å½•éŸ³")

        def _record_callback(indata, frames, time_info, status):
            if self._recording:
                self._recorded_frames.append(indata.copy())

        self._audio_stream = sd.InputStream(
            samplerate=RECORD_SAMPLE_RATE,
            channels=1,
            dtype='float32',
            callback=_record_callback,
        )
        self._audio_stream.start()

    def _on_voice_key_release(self):
        """Ctrl+Alt+A æ¾å¼€ â†’ åœæ­¢å½•éŸ³ï¼Œå¯åŠ¨ ASRâ†’LLMâ†’TTS æµæ°´çº¿"""
        if not self._recording:
            return
        self._recording = False
        print("[Voice] åœæ­¢å½•éŸ³")

        try:
            self._audio_stream.stop()
            self._audio_stream.close()
        except Exception:
            pass

        if not self._recorded_frames:
            self.comm.voice_status.emit("æœªæ£€æµ‹åˆ°éŸ³é¢‘è¾“å…¥ã€‚")
            return

        audio_data = np.concatenate(self._recorded_frames, axis=0).flatten()
        self._recorded_frames = []

        # åå°æ‰§è¡Œ ASR â†’ LLM â†’ TTS
        threading.Thread(target=self._voice_pipeline, args=(audio_data,), daemon=True).start()

    def _voice_pipeline(self, audio_data: np.ndarray):
        """è¯­éŸ³å¯¹è¯å…¨æµç¨‹: ASR â†’ LLM â†’ TTS â†’ æ’­æ”¾"""
        try:
            # --- 1) ASR: è¯­éŸ³è½¬æ–‡å­— ---
            self.comm.voice_status.emit("æ­£åœ¨è¯†åˆ«è¯­éŸ³...")
            tmp_wav = os.path.join(tempfile.gettempdir(), "_voice_input.wav")
            sf.write(tmp_wav, audio_data, RECORD_SAMPLE_RATE)

            results = self.asr_model.transcribe(
                audio=tmp_wav,
                language=None,
            )
            user_text = results[0].text.strip()
            detected_lang = results[0].language
            print(f"[Voice ASR] è¯­è¨€={detected_lang}, æ–‡å­—={user_text}")

            if not user_text:
                self.comm.voice_status.emit("æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³ã€‚")
                return

            # æ˜¾ç¤ºè¯†åˆ«ç»“æœ
            self.comm.append_chat.emit("Me ğŸ¤", user_text)

            # --- 2) LLM ---
            llm_input = user_text + "\nï¼ˆå›å¤ä¸­å°½é‡ä¸è¦å‡ºç°ç‰¹æ®Šç¬¦å·ï¼Œä¾¿äºæœ—è¯»ï¼‰"
            self.chat_history.append({'role': 'user', 'content': llm_input})
            try:
                web_broadcast("Me ğŸ¤", user_text)
            except Exception:
                pass

            response = self.client.chat(
                model=MODEL_NAME,
                messages=self.chat_history,
                tools=self.tools,
                keep_alive=-1
            )
            message = response.get('message', {})

            # å¤„ç†å·¥å…·è°ƒç”¨
            if message.get('tool_calls'):
                self.chat_history.append(message)
                for tool_call in message['tool_calls']:
                    t_name = tool_call['function']['name']
                    t_args = tool_call['function']['arguments']
                    print(f"[MCP Action] æ­£åœ¨è°ƒç”¨å·¥å…·: {t_name} å‚æ•°: {t_args}")
                    output = asyncio.run(self.call_mcp_tool(t_name, t_args))
                    self.chat_history.append({'role': 'tool', 'content': str(output), 'name': t_name})
                final_response = self.client.chat(model=MODEL_NAME, messages=self.chat_history)
                ai_content = final_response['message']['content']
                self.chat_history.append(final_response['message'])
            else:
                self.chat_history.append(message)
                ai_content = message.get('content', '')

            self.comm.append_chat.emit("AI", ai_content)
            try:
                web_broadcast("AI", ai_content)
            except Exception:
                pass

            # --- 3) TTS: åˆ†å¥æµå¼åˆæˆ + OutputStream æµå¼æ’­æ”¾ ---
            if ai_content.strip():
                sentences = split_sentences_for_tts(ai_content, TTS_TOKEN_MAX_NUM)
                print(f"[Voice TTS] æ‹†åˆ†ä¸º {len(sentences)} æ®µ: {sentences}")
                if not sentences:
                    return

                tts_lang = TTS_LANGUAGE
                # éŸ³é¢‘å—é˜Ÿåˆ—ï¼šå­˜æ”¾ np.ndarray ç‰‡æ®µï¼ŒNone ä¸ºç»“æŸå“¨å…µ
                audio_chunk_queue = queue.Queue(maxsize=64)
                SENTINEL = None
                # ç”¨äºåœ¨å›è°ƒä¸ç”Ÿäº§è€…ä¹‹é—´ä¼ é€’é‡‡æ ·ç‡
                sr_holder = [None]
                sr_ready = threading.Event()

                def tts_producer():
                    """é€å¥åˆæˆ TTSï¼Œå°†éŸ³é¢‘æŒ‰å°å—æ¨å…¥é˜Ÿåˆ—"""
                    CHUNK_SAMPLES = 4800  # çº¦ 200ms @24kHz
                    for i, sentence in enumerate(sentences):
                        try:
                            self.comm.voice_status.emit(f"æ­£åœ¨åˆæˆè¯­éŸ³ ({i+1}/{len(sentences)})...")
                            wavs, sr = self.tts_model.generate_custom_voice(
                                text=sentence,
                                language=tts_lang,
                                speaker=TTS_SPEAKER,
                            )
                            wav = wavs[0]
                            # é¦–æ¬¡æ‹¿åˆ° sr åé€šçŸ¥æ’­æ”¾çº¿ç¨‹
                            if sr_holder[0] is None:
                                sr_holder[0] = sr
                                sr_ready.set()
                            # å°†æ•´æ®µéŸ³é¢‘åˆ‡æˆå°å—æ¨å…¥é˜Ÿåˆ—
                            offset = 0
                            while offset < len(wav):
                                chunk = wav[offset:offset + CHUNK_SAMPLES]
                                audio_chunk_queue.put(chunk)
                                offset += CHUNK_SAMPLES
                            print(f"[Voice TTS] åˆæˆå®Œæˆ ({i+1}/{len(sentences)}): {sentence}")
                        except Exception as e:
                            print(f"[Voice TTS] åˆæˆç¬¬ {i+1} æ®µå¤±è´¥: {e}")
                    audio_chunk_queue.put(SENTINEL)

                def audio_player():
                    """ä½¿ç”¨ sd.OutputStream ä»é˜Ÿåˆ—æµå¼æ’­æ”¾éŸ³é¢‘"""
                    # ç­‰å¾…ç¬¬ä¸€æ®µåˆæˆå®Œæˆä»¥è·å–é‡‡æ ·ç‡
                    sr_ready.wait()
                    sr = sr_holder[0]
                    PLAYBACK_BLOCK = 1024  # OutputStream æ¯æ¬¡å›è°ƒçš„å¸§æ•°

                    # æ’­æ”¾ç¼“å†²åŒºï¼šç”¨ä¸€ä¸ª deque å¼çš„æ»šåŠ¨ buffer
                    buffer = np.array([], dtype=np.float32)
                    finished = False  # ç”Ÿäº§è€…æ˜¯å¦å·²å‘é€ SENTINEL

                    def callback(outdata, frames, time_info, status):
                        nonlocal buffer, finished
                        needed = frames
                        # å°è¯•ä»é˜Ÿåˆ—è¡¥å…… buffer
                        while len(buffer) < needed and not finished:
                            try:
                                chunk = audio_chunk_queue.get_nowait()
                                if chunk is SENTINEL:
                                    finished = True
                                    break
                                buffer = np.concatenate([buffer, chunk.astype(np.float32)])
                            except queue.Empty:
                                break

                        if len(buffer) >= needed:
                            outdata[:, 0] = buffer[:needed]
                            buffer = buffer[needed:]
                        else:
                            # buffer ä¸è¶³ï¼Œå¡«å……å·²æœ‰æ•°æ® + é™éŸ³
                            avail = len(buffer)
                            outdata[:avail, 0] = buffer[:avail]
                            outdata[avail:, 0] = 0.0
                            buffer = np.array([], dtype=np.float32)
                            if finished:
                                raise sd.CallbackStop()

                    with sd.OutputStream(
                        samplerate=sr,
                        channels=1,
                        dtype='float32',
                        blocksize=PLAYBACK_BLOCK,
                        callback=callback,
                    ) as stream:
                        # é˜»å¡ç›´åˆ°æ’­æ”¾ç»“æŸï¼ˆCallbackStop è§¦å‘ï¼‰
                        while stream.active:
                            # åœ¨éå›è°ƒçº¿ç¨‹ä¸­ä¹Ÿå¸®å¿™å¡«å…… bufferï¼Œé¿å…å›è°ƒé¥¥é¥¿
                            if not finished:
                                try:
                                    chunk = audio_chunk_queue.get(timeout=0.05)
                                    if chunk is SENTINEL:
                                        finished = True
                                    else:
                                        buffer = np.concatenate([buffer, chunk.astype(np.float32)])
                                except queue.Empty:
                                    pass
                            else:
                                # ç”Ÿäº§å®Œæ¯•ï¼Œç­‰å¾…æ’­æ”¾çº¿ç¨‹æ’ç©º buffer
                                sd.sleep(50)
                    print("[Voice TTS] OutputStream æ’­æ”¾ç»“æŸ")

                # å¯åŠ¨ç”Ÿäº§è€…å’Œæ’­æ”¾çº¿ç¨‹
                producer_thread = threading.Thread(target=tts_producer, daemon=True)
                player_thread = threading.Thread(target=audio_player, daemon=True)
                producer_thread.start()
                player_thread.start()

                producer_thread.join()
                player_thread.join()
                self.comm.voice_status.emit("è¯­éŸ³æ’­æ”¾å®Œæ¯•ã€‚")

        except Exception as e:
            self.comm.voice_status.emit(f"è¯­éŸ³å¤„ç†å¼‚å¸¸: {e}")
            print(f"[Voice] å¼‚å¸¸: {e}")

    def handle_exit(self):
        print("åŠ©æ‰‹æ­£åœ¨é€€å‡º...")
        QApplication.quit()

    def run_hotkey_listener(self):
        keyboard.add_hotkey('ctrl+alt+q', lambda: self.comm.trigger_show.emit())
        keyboard.add_hotkey('ctrl+alt+e', lambda: self.comm.request_exit.emit())
        # è¯­éŸ³å¿«æ·é”®ï¼šæŒ‰ä¸‹å¼€å§‹å½•éŸ³ï¼Œæ¾å¼€åœæ­¢
        keyboard.on_press_key('a', lambda e: self._on_voice_key_press() if keyboard.is_pressed('ctrl') and keyboard.is_pressed('alt') else None)
        keyboard.on_release_key('a', lambda e: self._on_voice_key_release() if not keyboard.is_pressed('a') else None)
        print("åŠ©æ‰‹å·²å¯åŠ¨ (Ctrl+Alt+Q å”¤èµ·, Ctrl+Alt+E é€€å‡º, Ctrl+Alt+A è¯­éŸ³å¯¹è¯)")

if __name__ == "__main__":
    app = QApplication(sys.argv)
    assistant = AIAssistant()
    assistant.run_hotkey_listener()

    # å¯åŠ¨ Web Chat æœåŠ¡ï¼ˆå±€åŸŸç½‘å¯è®¿é—®ï¼‰
    set_assistant(assistant)
    start_web_server(host="0.0.0.0", port=5100)

    sys.exit(app.exec())
